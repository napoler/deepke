# lm_name = 'bert-base-chinese'  # download usage
# cache file usage
lm_file: 'bert_pretrained'
# transformer 层数，初始 base bert 为12层
# 但是数据量较小时调低些反而收敛更快效果更好
num_hidden_layers: 2